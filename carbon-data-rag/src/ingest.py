"""
Ingestion DEFRA 2024 â†’ ChromaDB

Parse le fichier Excel DEFRA, structure les facteurs d'Ã©mission en documents,
et les vectorise dans ChromaDB pour recherche sÃ©mantique.

Usage:
    python ingest.py
"""

import pandas as pd
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pathlib import Path
from typing import List, Dict
import json
from tqdm import tqdm

# Configuration
DATA_DIR = Path(__file__).parent.parent / "data"
DEFRA_FILE = DATA_DIR / "defra_2024.xlsx"
CHROMA_DIR = DATA_DIR / "chroma_db"

# ModÃ¨le d'embeddings local (gratuit, rapide, performant)
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

class DEFRAIngester:
    """Parse et vectorise les donnÃ©es DEFRA dans ChromaDB"""
    
    def __init__(self):
        print("ğŸš€ Initialisation DEFRAIngester...")
        
        # CrÃ©er rÃ©pertoires si nÃ©cessaire
        DATA_DIR.mkdir(exist_ok=True)
        CHROMA_DIR.mkdir(exist_ok=True)
        
        # Charger le modÃ¨le d'embeddings
        print(f"ğŸ“¦ Chargement du modÃ¨le d'embeddings : {EMBEDDING_MODEL}")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
        
        # Initialiser ChromaDB en mode persistent
        self.chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))
        
        # CrÃ©er ou rÃ©cupÃ©rer la collection
        self.collection = self.chroma_client.get_or_create_collection(
            name="carbon_factors",
            metadata={"description": "DEFRA 2024 emission factors"}
        )
        
        print(f"âœ… Collection 'carbon_factors' prÃªte ({self.collection.count()} documents)")
    
    def parse_defra_sheet(self, sheet_name: str, df: pd.DataFrame, category: str) -> List[Dict]:
        """
        Parse un onglet DEFRA et extrait les facteurs structurÃ©s
        
        Returns:
            Liste de documents avec: text, metadata, id
        """
        documents = []
        
        # Nettoyer les colonnes
        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
        
        # Identifier la colonne du facteur CO2e (varie selon les onglets)
        co2e_columns = [col for col in df.columns if 'kg_co2e' in col or 'co2e' in col]
        if not co2e_columns:
            print(f"  âš ï¸  Pas de colonne CO2e trouvÃ©e dans {sheet_name}")
            return documents
        
        main_co2e_col = co2e_columns[0]
        
        # Identifier colonnes descriptives
        desc_columns = [col for col in df.columns if 'level' in col or 'type' in col or 'description' in col]
        
        # ItÃ©rer sur les lignes
        for idx, row in df.iterrows():
            factor_value = row.get(main_co2e_col)
            
            # Skip si pas de valeur
            if pd.isna(factor_value) or factor_value == 0:
                continue
            
            # Construire la description textuelle
            desc_parts = []
            for col in desc_columns:
                val = row.get(col)
                if pd.notna(val) and str(val).strip():
                    desc_parts.append(str(val).strip())
            
            description = " - ".join(desc_parts) if desc_parts else f"{category} emission factor"
            
            # Identifier l'unitÃ©
            unit_columns = [col for col in df.columns if 'unit' in col]
            unit = row.get(unit_columns[0]) if unit_columns else "per unit"
            if pd.isna(unit):
                unit = "per unit"
            
            # CrÃ©er le document pour RAG
            text = f"""
            Category: {category}
            Description: {description}
            Factor: {factor_value} kg CO2e {unit}
            """
            
            # MÃ©tadonnÃ©es structurÃ©es pour filtrage et retour
            metadata = {
                "category": category,
                "description": description,
                "factor": float(factor_value),
                "unit": str(unit),
                "source": "DEFRA 2024",
                "sheet": sheet_name,
                "raw_data": json.dumps({k: str(v) for k, v in row.to_dict().items() if pd.notna(v)}, ensure_ascii=False)[:500]
            }
            
            doc_id = f"{category}_{sheet_name}_{idx}"
            
            documents.append({
                "id": doc_id,
                "text": text.strip(),
                "metadata": metadata
            })
        
        return documents
    
    def ingest_defra(self):
        """Parse complet du fichier DEFRA et ingestion dans ChromaDB"""
        
        if not DEFRA_FILE.exists():
            print(f"âŒ Fichier DEFRA introuvable : {DEFRA_FILE}")
            print("\nğŸ“¥ TÃ©lÃ©chargez-le depuis :")
            print("https://www.gov.uk/government/publications/greenhouse-gas-reporting-conversion-factors-2024")
            print(f"â†’ Sauvegardez-le dans : {DEFRA_FILE}")
            return False
        
        print(f"\nğŸ“– Lecture DEFRA : {DEFRA_FILE}")
        
        # Onglets Ã  ingÃ©rer avec leurs catÃ©gories
        sheets_config = {
            'Passenger vehicles': 'transport',
            'Delivery vehicles': 'transport',
            'Flights': 'transport',
            'Rail': 'transport',
            'Sea': 'transport',
            'Fuels': 'energy',
            'Electricity': 'electricity',
            'Material waste': 'materials',
            'Water supply': 'water',
            'Water treatment': 'water'
        }
        
        all_documents = []
        
        for sheet_name, category in sheets_config.items():
            try:
                print(f"\n  ğŸ”„ Traitement : {sheet_name}")
                df = pd.read_excel(DEFRA_FILE, sheet_name=sheet_name, header=0)
                
                docs = self.parse_defra_sheet(sheet_name, df, category)
                all_documents.extend(docs)
                
                print(f"     âœ… {len(docs)} facteurs extraits")
                
            except Exception as e:
                print(f"     âš ï¸  Erreur sur {sheet_name}: {e}")
                continue
        
        if not all_documents:
            print("\nâŒ Aucun document extrait !")
            return False
        
        print(f"\nğŸ“Š Total : {len(all_documents)} facteurs d'Ã©mission extraits")
        
        # Vectorisation et ingestion dans ChromaDB
        print(f"\nğŸ”® GÃ©nÃ©ration des embeddings ({EMBEDDING_MODEL})...")
        
        texts = [doc["text"] for doc in all_documents]
        ids = [doc["id"] for doc in all_documents]
        metadatas = [doc["metadata"] for doc in all_documents]
        
        print("  Embedding en cours...")
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True, batch_size=32)
        
        print("\nğŸ’¾ Ingestion dans ChromaDB...")
        
        BATCH_SIZE = 5000
        for i in range(0, len(all_documents), BATCH_SIZE):
            batch_end = min(i + BATCH_SIZE, len(all_documents))
            
            self.collection.add(
                ids=ids[i:batch_end],
                embeddings=embeddings[i:batch_end].tolist(),
                documents=texts[i:batch_end],
                metadatas=metadatas[i:batch_end]
            )
            
            print(f"  âœ… Batch {i//BATCH_SIZE + 1} ingÃ©rÃ© ({batch_end}/{len(all_documents)})")
        
        print(f"\nğŸ‰ Ingestion terminÃ©e ! {self.collection.count()} documents dans ChromaDB")
        print(f"ğŸ“‚ Base vectorielle : {CHROMA_DIR}")
        
        return True
    
    def test_retrieval(self):
        """Test rapide de rÃ©cupÃ©ration"""
        print("\nğŸ§ª Test de rÃ©cupÃ©ration...")
        
        test_queries = [
            "Ã©missions d'une voiture Ã©lectrique",
            "electricity emissions in France",
            "vol court courrier avion"
        ]
        
        for query in test_queries:
            print(f"\n  Query: '{query}'")
            
            query_embedding = self.embedding_model.encode([query])[0]
            
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=2
            )
            
            if results['documents']:
                for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                    print(f"    {i+1}. {metadata.get('description', 'N/A')}")
                    print(f"       Factor: {metadata.get('factor', 'N/A')} {metadata.get('unit', '')}")
                    print(f"       Category: {metadata.get('category', 'N/A')}")


def main():
    """Point d'entrÃ©e principal"""
    
    print("="*80)
    print("  DEFRA 2024 â†’ ChromaDB Ingestion")
    print("="*80)
    
    ingester = DEFRAIngester()
    
    if ingester.collection.count() > 0:
        print(f"\nâš ï¸  La collection contient dÃ©jÃ  {ingester.collection.count()} documents")
        response = input("Voulez-vous rÃ©ingÃ©rer (Ã©crase les donnÃ©es) ? (y/N): ")
        
        if response.lower() == 'y':
            print("ğŸ—‘ï¸  Suppression de la collection existante...")
            ingester.chroma_client.delete_collection("carbon_factors")
            ingester = DEFRAIngester()
        else:
            print("â†ªï¸  Passage au test de rÃ©cupÃ©ration...")
            ingester.test_retrieval()
            return
    
    success = ingester.ingest_defra()
    
    if success:
        ingester.test_retrieval()
        
        print("\n" + "="*80)
        print("âœ… Module carbon-data-rag prÃªt !")
        print("="*80)
        print("\nğŸš€ Prochaine Ã©tape : lancer l'API")
        print("   $ fastapi dev src/api.py")
    else:
        print("\nâŒ Ã‰chec de l'ingestion")


if __name__ == "__main__":
    main()
